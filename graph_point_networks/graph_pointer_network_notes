Summary

1. Builds upon the pointer networks by adding a graph embedding layer on the input, which captures relationships between nodes
2. We train hierarchical gpns (HGPNs) -> learns a hierarchical policy to find an optimal city permutation under constraints
-> each leayr of the hierachy is designed with seperate reward function
3. Each layer of the hierarchy learns to search the feasible solutions under constraints
or learns the heuristic to optimize the objective function
4. each layer of the hierarchy defines a policy from which we sample actions. Each layer uses a hidden state h from the previous layer
each layer corresponds to a different rl task-> so the reward functions are hand-designed to be different for each layer

Results:
1. GPNs trained on small-scale TSP50/100 problems generalize well to larger scale TSP500/1000 problems with shorter tour len and faster
computational times
